# LLM Provider Selection (only ollama supported)
LLM_PROVIDER=ollama

# Ollama configuration (for local LLM inference)
OLLAMA__BASE_URL=http://localhost:11434
OLLAMA__MODEL=llama3.1
OLLAMA__TIMEOUT=120
# OLLAMA__API_KEY=optional_bearer_token